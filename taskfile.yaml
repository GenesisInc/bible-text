version: 3

vars:
  # Remove comma between numbers; replace all other commas with a space
  COMMA_PAT: '
    s/\([0-9]\),\([0-9]\)/\1\2/g;
    s/,/ /g
    '
  SPACE_PAT: "s/ / /g; s/;/ /g; s/[[:space:]][[:space:]]*/ /g; s/ $//g"
  SEMICOLON_PAT: "s/;/ /g"
  PLAIN_BASE_DIR: newWorldTranslation/english/2013-release-plain-text
  RAW_BASE_DIR: newWorldTranslation/english/2013-release
  FILE_PAT: "{{.PLAIN_BASE_DIR}}/*/*"

tasks:
  default: task -a
  run: task -p tag lifespan

  # newer version

  generate-json:
    cmds:
      - |
        uv run bible.py --generate

  extract-sample:
    cmds:
      - |
        uv run bible.py \
          --extract \
          --bible-json "bible_data.json" \
          --output-json "bible_entities.json" \
          --output-csv "bible_entities.csv" \
          --books john

  extract:
    cmds:
      - |
        uv run bible.py \
          --extract \
          --bible-json "bible_data.json" \
          --output-json "bible_entities.json" \
          --output-csv "bible_entities.csv"

  summary:
    cmds:
      - |
        mlr --csv --from bible_entities.csv count-distinct -f Type

  org:
    cmds:
      - |
        mlr --csv --from bible_entities.csv \
          filter '$Type == "ORG"' \
            then cut -f Text \
            | sort \
            | uniq -c

  occupation:
    cmds:
      - |
        mlr --c2p --barred --from bible_entities.csv \
          filter '$Type == "OCCUPATION"' \
            then count-distinct -f Text

  gpe:
    cmds:
      - |
        mlr --csv --from bible_entities.csv \
          filter '$Type == "GPE"' \
            then cut -f Text \
            | sort \
            | uniq -c

  norp:
    cmds:
      - |
        mlr --csv --from bible_entities.csv \
          filter '$Type == "NORP"' \
            then cut -f Text \
            | sort \
            | uniq -c

  names:
    cmds:
      - |
        mlr --csv --from bible_entities.csv \
          filter '$Type == "PERSON"' \
            then cut -f Text \
            | sort \
            | uniq -c

  date:
    cmds:
      - |
        mlr --csv --from bible_entities.csv \
          filter '$Type == "DATE"' \
            then cut -f Text \
            | sort \
            | uniq -c

  # older version

  tag2:
    parellel: true
    cmds:
      - uv run main.py --tag-entities

  names2:
    aliases:
      - persons
    cmds:
      - |
        mlr --csv --from analysis/entities.csv \
          filter '$Entity_Type=="PERSON"' \
            then cut -f Entity_Name \

  time2:
    cmds:
      - |
        mlr --csv --from analysis/entities.csv \
          filter '$Entity_Type=="TIME"' \
            then cut -f Entity_Name \
          | sort | uniq -c | sort -k1 -n

  lifespan:
    parellel: true
    cmds:
      - uv run main.py --extract-lifespan

  wc:
    cmds:
      - |
        wc {{.FILE_PAT}}

  refresh-plain-text:
    cmds:
      - |
        rm -rf {{.PLAIN_BASE_DIR}}
        cp -r {{.RAW_BASE_DIR}} {{.PLAIN_BASE_DIR}}

  clean-symbols:
    cmds:
      - task: semicolon
      - task: comma
      - task: spaces
      - task: remove-symbols
      - task: square-brackets
      - task: special-cases

  special-cases:
    cmds:
      - |
        # Remove only leading or trailing Unicode hyphens (EN DASH and EM DASH), but keep internal hyphens intact
        sed -i "" 's/^[\–—]\+//; s/[\–—]\+$//' {{.FILE_PAT}}

  verify-special-cases:
    cmds:
      - |
        words=("—He" "—I" "2" "—Let" "Mountains" "—Sihon" "—The" "—provided" "—that" "——" "perished—the")

        for word in "${words[@]}" ; do
          echo "$word" | sed 's/^[\–—]\+//; s/[\–—]\+$//'
        done

  square-brackets_2:
    cmds:
      - |
        # cleanup these words
        #   [Ayin]In [Ayin]They [Beth]In [Beth]Who [Daleth]And [Daleth]They
        #   [Lamed]12 [Lamed]By [Nun]He [Nun]Trustworthy [Waw]And
        sed -i "" 's/\[.*\]//g;' {{.FILE_PAT}}

  square-brackets:
    cmds:
      - |
        # Remove text within square brackets
        sed -i "" 's/\[[^]]*\]//g' {{.FILE_PAT}}

  remove-unicode-symbols:
    cmds:
      - |
        # Remove symbols including additional Unicode symbols
        sed -i "" 's/[()“”.;:‘ʹ·"!?’בדויִלנכע]//g' {{.FILE_PAT}}

  remove-symbols:
    cmds:
      - |
        # remove other symbols
        sed -i "" 's/[()“”.;:‘ʹ·"!?’]//g;' {{.FILE_PAT}}

  spaces:
    cmds:
      - |
        sed -i "" "{{.SPACE_PAT}}" {{.FILE_PAT}}

  semicolon:
    cmds:
      - |
        sed -i "" "{{.SEMICOLON_PAT}}" {{.FILE_PAT}}

  comma:
    cmds:
      - |
        sed -i "" "{{.COMMA_PAT}}" {{.FILE_PAT}}

  split:
    cmds:
      - |
        mkdir -p tmp
        # Split words and numbers
        cat {{.FILE_PAT}} | tr ' ' '\n' > analysis/words.list

        # Separate numbers and words
        grep '^[0-9]' analysis/words.list > tmp/numeric.list
        grep '^[^0-9]' analysis/words.list > tmp/words_only.list

        # Process numeric list: sort numerically and count occurrences
        cat tmp/numeric.list | sort -n | uniq -c | sort -k2,2n > tmp/numeric-counts.list

        # Process words list: sort by count (desc), then alphabetically
        cat tmp/words_only.list | sort | uniq -c | sort -k1,1nr -k2,2 > tmp/words-counts.list

        # Combine both lists: numbers first, then words
        cat tmp/words-counts.list tmp/numeric-counts.list > analysis/word-counts.list

  fix-verse-separator:
    cmds:
      - |
        for file in newWorldTranslation/english/2013-release/*/*; do
          sed -E -i '' 's/^1 /1 /' "$file"
        done

  split-2:
    cmds:
      - |
        cat {{.FILE_PAT}}  | tr ' ' '\n' > analysis/words.list
        cat analysis/words.list | sort | uniq -c | sort -n > analysis/word-counts.list

  analyze:
    cmds:
      - |
        cat -n analysis/word-counts.list

  verify:
    cmds:
      - |
        words=("day,16" "saying,May" "10,100" "10,x" "x,10" "them,Because" "multi   spaces" "greedy one; [Nun]He")

        for word in "${words[@]}" ; do
          echo "$word" \
            | sed "{{.COMMA_PAT}}" \
            | sed "{{.SPACE_PAT}}"
        done
